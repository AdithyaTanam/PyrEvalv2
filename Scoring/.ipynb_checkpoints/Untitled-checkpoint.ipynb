{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESHOLD 80\n",
      "\ta: 250\tb: 1.0\n",
      "\t\t881197544\n",
      "\t\t881520246\n",
      "\t\t881745597\n",
      "\t\t883076719\n",
      "\t\t883178560\n",
      "\t\t883690042\n",
      "\t\t883707584\n",
      "\t\t884203758\n",
      "\t\t885640486\n",
      "\t\t885925399\n",
      "\t\t887016576\n",
      "\t\t887051477\n",
      "\t\t887582679\n",
      "\t\t887645490\n",
      "\t\t887897589\n",
      "\t\t887898669\n",
      "\t\t888277107\n",
      "\t\t888717500\n",
      "\t\t889198081\n",
      "\t\t889920032\n",
      "\ta: 250\tb: 1.5\n",
      "\t\t881197544\n",
      "\t\t881520246\n",
      "\t\t881745597\n",
      "\t\t883076719\n",
      "\t\t883178560\n",
      "\t\t883690042\n",
      "\t\t883707584\n",
      "\t\t884203758\n",
      "\t\t885640486\n",
      "\t\t885925399\n",
      "\t\t887016576\n",
      "\t\t887051477\n",
      "\t\t887582679\n",
      "\t\t887645490\n",
      "\t\t887897589\n",
      "\t\t887898669\n",
      "\t\t888277107\n",
      "\t\t888717500\n",
      "\t\t889198081\n",
      "\t\t889920032\n",
      "\ta: 250\tb: 2.0\n",
      "\t\t881197544\n",
      "\t\t881520246\n",
      "\t\t881745597\n",
      "\t\t883076719\n",
      "\t\t883178560\n",
      "\t\t883690042\n",
      "\t\t883707584\n",
      "\t\t884203758\n",
      "\t\t885640486\n",
      "\t\t885925399\n",
      "\t\t887016576\n",
      "\t\t887051477\n",
      "\t\t887582679\n",
      "\t\t887645490\n",
      "\t\t887897589\n",
      "\t\t887898669\n",
      "\t\t888277107\n",
      "\t\t888717500\n",
      "\t\t889198081\n",
      "\t\t889920032\n",
      "\ta: 250\tb: 2.5\n",
      "\t\t881197544\n",
      "\t\t881520246\n",
      "\t\t881745597\n",
      "\t\t883076719\n",
      "\t\t883178560\n",
      "\t\t883690042\n",
      "\t\t883707584\n",
      "\t\t884203758\n",
      "\t\t885640486\n",
      "\t\t885925399\n",
      "\t\t887016576\n",
      "\t\t887051477\n",
      "\t\t887582679\n",
      "\t\t887645490\n",
      "\t\t887897589\n",
      "\t\t887898669\n",
      "\t\t888277107\n",
      "\t\t888717500\n",
      "\t\t889198081\n",
      "\t\t889920032\n",
      "\ta: 250\tb: 3.0\n",
      "\t\t881197544\n",
      "\t\t881520246\n",
      "\t\t881745597\n",
      "\t\t883076719\n",
      "\t\t883178560\n",
      "\t\t883690042\n",
      "\t\t883707584\n",
      "\t\t884203758\n",
      "\t\t885640486\n",
      "\t\t885925399\n",
      "\t\t887016576\n",
      "\t\t887051477\n",
      "\t\t887582679\n",
      "\t\t887645490\n",
      "\t\t887897589\n",
      "\t\t887898669\n",
      "\t\t888277107\n",
      "\t\t888717500\n",
      "\t\t889198081\n",
      "\t\t889920032\n"
     ]
    }
   ],
   "source": [
    "# %load processing.py\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import copy\n",
    "import statistics \n",
    "import glob\n",
    "import csv\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "===================== SCU ======================\n",
    "\"\"\"\n",
    "\n",
    "class SCU():\n",
    "    def __init__(self, scu_id, weight, segment_embeddings):\n",
    "        self.id = scu_id\n",
    "        self.embeddings = segment_embeddings\n",
    "        self.weight = weight\n",
    "    def averageSimilarity(self, segment_embedding):\n",
    "        normalizer = len(self.embeddings)\n",
    "        similarity = 0\n",
    "        for embedding in self.embeddings:\n",
    "            similarity += cos(embedding, segment_embedding)[0][0]\n",
    "        return [similarity / normalizer, self.weight]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "============== Sentence Graph ====================\n",
    "\"\"\"\n",
    "\n",
    "class SentenceGraph():\n",
    "    \"\"\" Given a Sentence, build a graph from segmentations \"\"\"\n",
    "    def __init__(self, sentence_id, segmentations, scus):\n",
    "        self.sentence_id = sentence_id\n",
    "        self.segmentations = segmentations\n",
    "        self.graph = self.buildGraph(scus)\n",
    "    def buildGraph(self, scus):\n",
    "        segmentations = self.segmentations\n",
    "        graph = []\n",
    "        for segmentation, segments in segmentations.items():\n",
    "            hypernode = []\n",
    "            for segment_id, segment_embedding in segments.items():\n",
    "                scu_list = self.findSCUs(segment_embedding, scus)\n",
    "                hypernode.append(Vertex(segment_id, scu_list))\n",
    "            graph.append(hypernode)\n",
    "        return graph\n",
    "    def findSCUs(self, segment_embedding, scus):\n",
    "        scores = {}\n",
    "        for scu in scus:\n",
    "            scores[scu.id] = scu.averageSimilarity(segment_embedding)\n",
    "        scores = sorted(scores.items(), key=lambda x:x[1][0], reverse=True)[:2]\n",
    "        scores = [(score[0], score[1][0]) for score in scores]\n",
    "        return scores\n",
    "\n",
    "class Vertex():\n",
    "    def __init__(self, segment_id, scu_list):\n",
    "        self.id = segment_id\n",
    "        self.scu_list = scu_list\n",
    "        self.neighbors = []\n",
    "        self.useMe = True\n",
    "    def getWeight(self):\n",
    "        ###### Weight Scheme 1\n",
    "        #return sum([scu[1] for scu in self.scu_list]) / len(self.scu_list)\n",
    "        ###### Weight Scheme 2\n",
    "        return sum([scu[0] for scu in self.scu_list])\n",
    "        ###### Weight Scheme 3\n",
    "        #return max([scu[1] for scu in self.scu_list])\n",
    "\n",
    "    def add_neighbor(self, neighbor):\n",
    "        self.neighbors.append(neighbor)\n",
    "        neighbor.we_are_neighbors(self)\n",
    "    def add_neighbors(self, *args):\n",
    "        for neighbor in args:\n",
    "            self.neighbors.append(neighbor)\n",
    "            neighbor.we_are_neighbors(self)\n",
    "    def we_are_neighbors(self, neighbor):\n",
    "        self.neighbors.append(neighbor)\n",
    "    def delete(self):\n",
    "        self.useMe = False\n",
    "        for neighbor in self.neighbors:\n",
    "            neighbor.useMe = False\n",
    "\n",
    "\n",
    "class SummaryGraph():\n",
    "    def __init__(self, sentences, scus):\n",
    "        self.sentences = [SentenceGraph(n, segmentations, scus) for n, segmentations in sentences.items()]\n",
    "        for sentence in self.sentences:\n",
    "            self.buildInnerEdgesList(sentence.graph)\n",
    "        self.vertices = self.buildOuterEdgesList()\n",
    "        self.independentSet = self.buildIndependentSet()\n",
    "    def buildInnerEdgesList(self, sentenceGraph):\n",
    "        nodes = list(sentenceGraph)\n",
    "        while len(nodes) > 0:\n",
    "            node = nodes[0]\n",
    "            for vertex in node:\n",
    "                for n in nodes[1:]:\n",
    "                    for vert in n:\n",
    "                        vertex.add_neighbor(vert)\n",
    "            nodes = nodes[1:]\n",
    "    def buildOuterEdgesList(self):\n",
    "        sentences = list(self.sentences)\n",
    "        vertex_list = []\n",
    "        while (len(sentences) > 0):\n",
    "            sentence = sentences[0]\n",
    "            for node in sentence.graph:\n",
    "                for vertex in node:\n",
    "                    vertex_list.append(vertex)\n",
    "                    for sent in sentences[1:]:\n",
    "                        for n in sent.graph:\n",
    "                            for vert in n:\n",
    "                                if set(vertex.scu_list) & set(vert.scu_list):\n",
    "                                    vertex.add_neighbor(vert)\n",
    "            sentences.remove(sentence)\n",
    "        return vertex_list\n",
    "    def buildIndependentSet(self):\n",
    "        vertices = copy.deepcopy([vertex for vertex in self.vertices if vertex.useMe == True])\n",
    "        independentSet = []\n",
    "        while len(vertices) != 0:\n",
    "            vertex = max(vertices, key=lambda x: x.getWeight())\n",
    "            independentSet.append(vertex)\n",
    "            vertex.delete()\n",
    "            vertices = [vert for vert in vertices if vert.useMe == True]\n",
    "        return independentSet\n",
    "\n",
    "\"\"\"\n",
    "============= Function Definitions ===============\n",
    "\"\"\"\n",
    "\n",
    "def buildSCUlist(scu_pickle):\n",
    "    with open(scu_pickle, 'rb') as f:\n",
    "        SCUs = pickle.load(f)\n",
    "    f.close()\n",
    "    scus =[]\n",
    "    for scu_id, weight_embeddings in SCUs.items():\n",
    "        scus.append(SCU(scu_id, int(weight_embeddings[0]), weight_embeddings[1]))\n",
    "    return scus\n",
    "\n",
    "def sentencesFromSegmentations(fname):\n",
    "    f = open(fname, 'r')\n",
    "    segments = f.readlines()\n",
    "    sentences = {}\n",
    "    for segment in segments:\n",
    "        segment = segment.split('&')\n",
    "        if segment[1] in sentences.keys():\n",
    "            embedding = segment[4].strip('\\n').replace('[', '').replace(']', '')\n",
    "            embedding = [float(i) for i in embedding.split(',')]\n",
    "            sentences[segment[1]].append({'&'.join(segment[:4]):embedding})\n",
    "        else:\n",
    "            embedding = segment[4].strip('\\n').replace('[', '').replace(']', '')\n",
    "            embedding = [float(i) for i in embedding.split(',')]\n",
    "            sentences[segment[1]] = [{'&'.join(segment[:4]):embedding}]\n",
    "    sentences = sorted(sentences.items(), key=lambda x: int(x[0]))\n",
    "    sentences = [sentence[1] for sentence in sentences]\n",
    "    sents = []\n",
    "    for sentence in sentences:\n",
    "        segmentations = {}\n",
    "        for segment in sentence:\n",
    "            for segment_id, embedding in segment.items():\n",
    "                if segment_id.split('&')[2] in segmentations.keys():\n",
    "                    segmentations[segment_id.split('&')[2]][segment_id] = embedding\n",
    "                else:\n",
    "                    segmentations[segment_id.split('&')[2]] = {}\n",
    "                    segmentations[segment_id.split('&')[2]][segment_id] = embedding\n",
    "        sents.append(segmentations)\n",
    "    return dict(enumerate(sents))\n",
    "\n",
    "def buildSCUcandidateList(vertices):\n",
    "    scu_and_segments = {}\n",
    "    vertices = sorted(vertices, key = lambda x: int(x.id.split('&')[1]))\n",
    "    for vertex in vertices:\n",
    "        for scu in vertex.scu_list:\n",
    "            if scu[0] in scu_and_segments.keys():\n",
    "                scu_and_segments[scu[0]][vertex.id] = scu[1]\n",
    "            else:\n",
    "                scu_and_segments[scu[0]] = {}\n",
    "                scu_and_segments[scu[0]][vertex.id] = scu[1]\n",
    "    return scu_and_segments\n",
    "\n",
    "def processResults(scu_and_segments, independentSet):\n",
    "    scu_and_segments = copy.deepcopy(scu_and_segments)\n",
    "    chosen = []\n",
    "    chosen_scus = []\n",
    "    segment_and_scu = {}\n",
    "    for scu, segments in scu_and_segments.items():\n",
    "        for segment in segments.keys():\n",
    "            if segment in chosen:\n",
    "                del segments[segment] \n",
    "        if len(segments) != 0:\n",
    "            median = statistics.median_high(segments.values())\n",
    "        for segment, value in segments.items():\n",
    "            if value == median:\n",
    "                segment_and_scu[segment] = scu\n",
    "                #print(segment, scu)\n",
    "                chosen.append(segment)\n",
    "                chosen_scus.append(scu)\n",
    "                del segments[segment]\n",
    "    return segment_and_scu\n",
    "\n",
    "def scusBySentences(segment_scu):\n",
    "    sentences = {}\n",
    "    for segment, scu in segment_scu.items():\n",
    "        sentence_id = segment.split('&')[1]\n",
    "        if sentence_id in sentences.keys():\n",
    "            sentences[sentence_id][segment] = scu\n",
    "        else:\n",
    "            sentences[sentence_id] = {}\n",
    "            sentences[sentence_id][segment] = scu\n",
    "    return sentences\n",
    "\n",
    "def getScore(sentences, scus):\n",
    "    sentence_scores = {}\n",
    "    matched_cus = 0\n",
    "    for sentence, segments in sentences.items():\n",
    "        lil_score = 0\n",
    "        for segment, scu in segments.items():\n",
    "            for s in scus:\n",
    "                if scu == s.id:\n",
    "                    lil_score += s.weight\n",
    "                    matched_cus += 1\n",
    "        sentence_scores[sentence] = lil_score\n",
    "    return sum(sentence_scores.values()), matched_cus\n",
    "\n",
    "def filename(fname):\n",
    "    slash = fname.rfind('/') + 1\n",
    "    dot = fname.rfind('.')\n",
    "    return fname[slash:dot]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "================== Scores and Results ================\n",
    "'''\n",
    "\n",
    "def recall(results, fname):\n",
    "    path = 'pan/op_' + filename(fname) + 'pan'\n",
    "    orig_scus = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line.split('\\t')\n",
    "            if type(line[0]) == int:\n",
    "                if line[0] not in orig_scus:\n",
    "                    orig_scus.append(line[0])\n",
    "\n",
    "\n",
    "'''\n",
    "=================== Pipeline ===========================\n",
    "'''\n",
    "\n",
    "# #fnames = sys.argv[1:]\n",
    "# name_score = {}\n",
    "\n",
    "# #WMIN_types = {'max': max, 'min': min, 'median_high':statistics.median_high, 'median_low': statistics.median_low}\n",
    "\n",
    "# results_file = '../results.csv'\n",
    "\n",
    "# f = open(results_file, 'w')\n",
    "# f.close()\n",
    "\n",
    "# fnames = list(glob.iglob('peer_summaries/*'))\n",
    "# pyramids = list(glob.iglob('pyramids/*'))\n",
    "\n",
    "# summary_score = {}\n",
    "# for fname in fnames:\n",
    "#     f = filename(fname)\n",
    "#     summary_score[f] = []\n",
    "\n",
    "# for pyramid in pyramids:\n",
    "#     print('Pyrmid: {}'.format(pyramid))\n",
    "\n",
    "#     for fname in fnames: \n",
    "#         print('\\tFilename: {}'.format(fname))\n",
    "\n",
    "#         scu_pickle = pyramid \n",
    "\n",
    "#         sentences = sentencesFromSegmentations(fname)\n",
    "#         scus = buildSCUlist(scu_pickle)\n",
    "\n",
    "#         Graph = SummaryGraph(sentences, scus)\n",
    "#         independentSet = Graph.independentSet\n",
    "\n",
    "#         candidates = buildSCUcandidateList(independentSet)\n",
    "#         results = processResults(candidates, independentSet)\n",
    "#         rearranged_results = scusBySentences(results)\n",
    "\n",
    "#         scores, score = getScore(rearranged_results, scus)\n",
    "\n",
    "#         #print('\\n\\nScores by Sentences for {}:'.format(fname))\n",
    "#         #for sentence, s in scores.items():\n",
    "#             #print('\\tSentence {}: {}'.format(sentence, s))\n",
    "#         #print('Overall Score: {}'.format(score))\n",
    "\n",
    "#         summary_score[filename(fname)].append(score)\n",
    "\n",
    "#         #print('\\n')\n",
    "\n",
    "# with open(results_file, 'a') as f:\n",
    "#     w = csv.writer(f)\n",
    "#     w.writerow(['Summary'] + [pyramid[pyramid.rfind('/')+ 1:] for pyramid in pyramids])\n",
    "#     for summary, scores in summary_score.items():\n",
    "#         w.writerow([s for s in scores])\n",
    "#         print([s for s in scores])\n",
    "\n",
    "\n",
    "\n",
    "raw_HUMAN_SCORES = [47,38,38,22,\n",
    "                    46,51,34,60,\n",
    "                    13,10,22,19,\n",
    "                    54,26,44,43,\n",
    "                    16,36,25,40]\n",
    "\n",
    "quality_HUMAN_SCORES = [0.7705,0.6552,0.5938,0.4231,\n",
    "                        0.7188,0.8361,0.7907,0.8571,\n",
    "                        0.3333,0.2857,0.5641,0.7037,\n",
    "                        0.7714,0.6667,0.8269,0.8,\n",
    "                        0.5161,0.6923,0.641,0.8163]\n",
    "\n",
    "coverage_HUMAN_SCORES = [0.5222,0.4222,0.4222,0.2444,\n",
    "                        0.5111,0.5667,0.3778,0.6667,\n",
    "                        0.1444,0.1111,0.2442,0.2111,\n",
    "                        0.6,0.2889,0.4778,0.4889,\n",
    "                        0.1778,0.4,0.2778,0.4444,]\n",
    "\n",
    "comprehension_HUMAN_SCORES = [0.6463,0.5387,0.508,0.3337,\n",
    "                            0.6149,0.7014,0.5842,0.7619,\n",
    "                            0.2389,0.1984,0.4042,0.4574,\n",
    "                            0.6857,0.4778,0.6523,0.6444,\n",
    "                            0.3469,0.5462,0.4594,0.6303]\n",
    "\n",
    "\n",
    "class thresholdTable():\n",
    "    def __init__(self, threshold, thresholdCells):\n",
    "        self.rawScoreTable = self.buildTable(thresholdCells, 'raw')\n",
    "        self.qualityScoreTable = self.buildTable(thresholdCells, 'quality')\n",
    "        self.coverageScoreTable = self.buildTable(thresholdCells, 'coverage')\n",
    "        self.comprehensionScoreTable = self.buildTable(thresholdCells, 'comprehension')\n",
    "\n",
    "    def buildTable(self, thresholdCells, tableType):\n",
    "        Table = {}\n",
    "        for cell in thresholdCells:\n",
    "            if cell.a in Table.keys():\n",
    "                Table[cell.a].append((cell.b, getattr(cell, tableType)))\n",
    "            else:\n",
    "                Table[cell.a] = []\n",
    "                Table[cell.a].append((cell.b, getattr(cell, tableType)))\n",
    "        for a, b_list in Table.items():\n",
    "            s_b_list = sorted(b_list, key=lambda x: x[0])\n",
    "            Table[a] = np.array([s_b[1] for s_b in s_b_list])\n",
    "        return Table\n",
    "\n",
    "\n",
    "class thresholdTableCell():\n",
    "    def __init__(self, threshold, a, b, raw_scores, quality_scores, coverage_scores, comprehension_scores, \n",
    "                                                raw_human_scores=raw_HUMAN_SCORES, quality_human_scores = quality_HUMAN_SCORES, \n",
    "                                                coverage_human_scores = coverage_HUMAN_SCORES, comprehension_human_scores = comprehension_HUMAN_SCORES):\n",
    "        self.__rawHumanScores = raw_human_scores\n",
    "        self.__qualityHumanScores = quality_human_scores\n",
    "        self.__coverageHumanScores = coverage_human_scores\n",
    "        self.__comprehensionHumanScores = comprehension_human_scores\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "        self.raw = self.__getRawCorrelation(raw_scores)\n",
    "        self.quality = self.__getQualityCorrelation(quality_scores)\n",
    "        self.coverage = self.__getCoverageCorrelation(coverage_scores)\n",
    "        self.comprehension = self.__getComprehensionCorrelation(comprehension_scores)\n",
    "\n",
    "\n",
    "    def __getRawCorrelation(self, raw_scores):\n",
    "        return pearsonr(self.__rawHumanScores, raw_scores)[0]\n",
    "    def __getQualityCorrelation(self, quality_scores):\n",
    "        return pearsonr(self.__qualityHumanScores, quality_scores)[0]\n",
    "    def __getCoverageCorrelation(self, coverage_scores):\n",
    "        return pearsonr(self.__coverageHumanScores, coverage_scores)[0]\n",
    "    def __getComprehensionCorrelation(self, comprehension_scores):\n",
    "        return pearsonr(self.__comprehensionHumanScores, comprehension_scores)[0]\n",
    "\n",
    "class specialCell():\n",
    "    def __init__(self, a, b, operation, data):\n",
    "        self.raw = self.TableByOperationType(operation, data)\n",
    "        self.quality = self.TableByOperationType(operation, data)\n",
    "        self.coverage = self.TableByOperationType(operation, data)\n",
    "        self.comprehension = self.TableByOperationType(operation, data)\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    def TableByOperationType(operation, data):\n",
    "        return operation(data)\n",
    "\n",
    "\n",
    "\n",
    "def maxRawScore(count_by_weight, num):\n",
    "    counts = sorted(count_by_weight.items(), key=lambda x:x[0], reverse=True)\n",
    "    result = 0\n",
    "    for count in counts:\n",
    "        if num >= count[1]:\n",
    "            num = num - count[1]\n",
    "            result = result + (count[0]*count[1])\n",
    "        else:\n",
    "            result = result + (num*count[0])\n",
    "            num = 0\n",
    "    return result\n",
    "\n",
    "def getLayerSizes(fname):\n",
    "    f = open(fname, 'r')\n",
    "    lines = f.readlines()\n",
    "    count_by_weight = {}\n",
    "    count = 0\n",
    "    for n, line in enumerate(lines):\n",
    "        count_by_weight[n + 1] = int(line.strip())\n",
    "        count += (n+1) * int(line.strip())\n",
    "    avg = count/(n+1)\n",
    "    return count_by_weight, avg\n",
    "\n",
    "def setDataFrame(table):\n",
    "    attrs = ['rawScoreTable', 'qualityScoreTable', 'coverageScoreTable', 'comprehensionScoreTable']\n",
    "    indices = ['1.0', '1.5', '2.0', '2.5', '3.0']\n",
    "    columns = ['125', '150', '175', '200', '225', '250']\n",
    "    for attribute in attrs:\n",
    "        df = pd.DataFrame(getattr(table, attribute), index=indices)\n",
    "        setattr(table, attribute, df)\n",
    "\n",
    "pyramids = list(glob.iglob('pyramids/readable_pyramid_t80_a250_*'))\n",
    "summaries = list(glob.iglob('peer_summaries/*'))\n",
    "\n",
    "pyramids_by_threshold = {}\n",
    "acc = {}\n",
    "for pyramid in pyramids:\n",
    "    threshold_indicator = pyramid.rfind('t') + 1\n",
    "    threshold = int(pyramid[threshold_indicator:(threshold_indicator + 2)])\n",
    "    if threshold in pyramids_by_threshold.keys():\n",
    "        pyramids_by_threshold[threshold].append(pyramid)\n",
    "    else:\n",
    "        pyramids_by_threshold[threshold] = []\n",
    "        acc[threshold] = {}\n",
    "        pyramids_by_threshold[threshold].append(pyramid)\n",
    "\n",
    "table_types = ['raw', 'quality', 'coverage', 'comprehension']\n",
    "a_vals = [125, 150, 175, 200, 225, 250]\n",
    "b_vals = [1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "\n",
    "for threshold in acc.keys():\n",
    "    for t_type in table_types:\n",
    "        acc[threshold][t_type] = {}\n",
    "        for a in a_vals:\n",
    "            acc[threshold][t_type][a] = {}\n",
    "            for b in b_vals:\n",
    "                acc[threshold][t_type][a][b] = []\n",
    "\n",
    "tables = {}\n",
    "for threshold, pyramids in pyramids_by_threshold.items():\n",
    "    print 'THRESHOLD {}'.format(threshold)\n",
    "    tableCells = []\n",
    "\n",
    "    for pyramid in pyramids:\n",
    "        a_ind = pyramid.rfind('a') + 1\n",
    "        a = int(pyramid[a_ind:(a_ind + 3)])\n",
    "        b_ind = pyramid.rfind('b') + 1\n",
    "        b = float(pyramid[b_ind:(b_ind + 3)])\n",
    "        print '\\ta: {}\\tb: {}'.format(a, b)\n",
    "        raw_scores = {}\n",
    "        quality_scores = {}\n",
    "        coverage_scores = {}\n",
    "        comprehension_scores = {}\n",
    "        for summary in summaries:\n",
    "            summary_slash= summary.rfind('/') + 1\n",
    "            summary_dot = summary.rfind('.')\n",
    "            summary_name = int(summary[summary_slash:summary_dot])\n",
    "            print '\\t\\t{}'.format(summary_name)\n",
    "            sentences = sentencesFromSegmentations(summary)\n",
    "            scus = buildSCUlist(pyramid)\n",
    "            Graph = SummaryGraph(sentences, scus)\n",
    "            independentSet = Graph.independentSet\n",
    "            candidates = buildSCUcandidateList(independentSet)\n",
    "            results = processResults(candidates, independentSet)\n",
    "            rearranged_results = scusBySentences(results)\n",
    "            score, matched_cus = getScore(rearranged_results, scus)\n",
    "            size_file = pyramid.replace('.p', '.size').replace('pyramids/', 'sizes/')\n",
    "            count_by_weight, avg = getLayerSizes(size_file)\n",
    "            raw_scores[summary_name] = score\n",
    "            quality = float(score)/maxRawScore(count_by_weight, matched_cus)\n",
    "            coverage = float(score)/maxRawScore(count_by_weight, avg)\n",
    "            comprehension = float((quality + coverage)) / 2\n",
    "            quality_scores[summary_name] = quality\n",
    "            coverage_scores[summary_name] = coverage\n",
    "            comprehension_scores[summary_name] = comprehension\n",
    "        raw_scores = sorted(raw_scores.items(), key=lambda x: x[0])\n",
    "        raw_scores = [s[1] for s in raw_scores]\n",
    "        quality_scores = sorted(quality_scores.items(), key=lambda x: x[0])\n",
    "        quality_scores = [s[1] for s in quality_scores]\n",
    "        coverage_scores = sorted(coverage_scores.items(), key=lambda x: x[0])\n",
    "        coverage_scores = [s[1] for s in coverage_scores]\n",
    "        comprehension_scores = sorted(comprehension_scores.items(), key=lambda x: x[0])\n",
    "        comprehension_scores = [s[1] for s in comprehension_scores]\n",
    "        thresholdCell = thresholdTableCell(threshold, a, b, raw_scores, quality_scores, coverage_scores, comprehension_scores)\n",
    "        tableCells.append(thresholdCell)\n",
    "        acc[threshold]['raw'][a][b].append(thresholdCell.raw)\n",
    "        acc[threshold]['quality'][a][b].append(thresholdCell.quality)\n",
    "        acc[threshold]['coverage'][a][b].append(thresholdCell.coverage)\n",
    "        acc[threshold]['comprehension'][a][b].append(thresholdCell.comprehension)\n",
    "    tables[threshold] = thresholdTable(threshold, tableCells)\n",
    "\n",
    "for threshold, table in tables.items():\n",
    "    setDataFrame(table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "operations = {'average': statistics.mean, 'variance': statistics.stdev}\n",
    "averageCells = []\n",
    "varianceCells = []\n",
    "for threshold, t_type in acc.items():\n",
    "    for t_type, a_b in t_type.items():\n",
    "        for a, b_list in a_b.items():\n",
    "            for b, values in b_list.items():\n",
    "                averageCells.append(specialCell(a, b, operations['average'], values))\n",
    "                varianceCells.append(specialCell(a, b, operations['variance'], values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
